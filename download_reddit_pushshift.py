#!/usr/bin/env python3
"""
Reddit Pushshift Downloader - UNLIMITED VIDEO DOWNLOADS
Uses Pushshift Archive API to bypass Reddit's 1,000 post limit
Can download 50,000+ videos per subreddit with NO rate limiting!
"""

import requests
import subprocess
import time
import json
from pathlib import Path
from tqdm import tqdm
import concurrent.futures
from threading import Lock
from datetime import datetime, timedelta

# ‚úÖ VERIFIED WORKING FIGHT SUBREDDITS
FIGHT_SUBREDDITS = [
    {'name': 'fightporn', 'cctv_percent': 70},
    {'name': 'PublicFreakout', 'cctv_percent': 40},
    {'name': 'CrazyFuckingVideos', 'cctv_percent': 55},
    {'name': 'AbruptChaos', 'cctv_percent': 45},
    {'name': 'StreetMartialArts', 'cctv_percent': 60},
    {'name': 'fights', 'cctv_percent': 50},
]

# Global progress tracking
download_lock = Lock()
download_stats = {'success': 0, 'failed': 0}

def get_reddit_posts_pushshift(subreddit, max_posts=50000):
    """
    Get posts from Pushshift Archive - NO 1,000 POST LIMIT!
    Pushshift is an academic archive with ALL Reddit posts
    """
    posts = []
    before = int(datetime.now().timestamp())

    print(f"  üîÑ Fetching from Pushshift Archive (UNLIMITED)...")

    # Pushshift API endpoint
    base_url = "https://api.pushshift.io/reddit/search/submission"

    # Fetch in batches of 100 (Pushshift limit per request)
    batch_size = 100
    batches_needed = (max_posts // batch_size) + 1

    for batch in range(batches_needed):
        if len(posts) >= max_posts:
            break

        params = {
            'subreddit': subreddit,
            'size': batch_size,
            'before': before,
            'sort': 'desc',  # Newest first, then go backwards in time
            'sort_type': 'created_utc',
        }

        try:
            response = requests.get(base_url, params=params, timeout=30)

            if response.status_code != 200:
                print(f"    ‚ö†Ô∏è  HTTP {response.status_code} from Pushshift")
                break

            data = response.json()

            if 'data' not in data or not data['data']:
                print(f"    ‚ÑπÔ∏è  No more posts available from Pushshift")
                break

            batch_posts = data['data']
            posts.extend(batch_posts)

            # Update 'before' timestamp for next batch (go further back in time)
            if batch_posts:
                before = batch_posts[-1]['created_utc']

            # Progress indicator
            if (batch + 1) % 10 == 0:
                print(f"    Batch {batch+1}: {len(posts)} posts so far...", end='\r')

            # Minimal delay (Pushshift is very tolerant)
            time.sleep(0.5)

        except Exception as e:
            print(f"    ‚ùå Error fetching batch {batch+1}: {e}")
            break

    print(f"    ‚úÖ Fetched {len(posts)} posts from Pushshift Archive")
    return posts

def extract_video_url_pushshift(post_data):
    """Extract video URL from Pushshift post data"""
    try:
        # Pushshift returns post data without the 'data' wrapper
        data = post_data

        # Check for video
        is_video = data.get('is_video', False)
        url = data.get('url', '')
        domain = data.get('domain', '')

        # Reddit hosted video
        if is_video and 'media' in data and data['media']:
            reddit_video = data['media'].get('reddit_video', {})
            if 'fallback_url' in reddit_video:
                return reddit_video['fallback_url']

        # External video hosts
        video_hosts = ['gfycat.com', 'imgur.com', 'youtube.com', 'youtu.be',
                      'streamable.com', 'v.redd.it', 'redgifs.com']

        if any(host in domain or host in url for host in video_hosts):
            # For v.redd.it URLs, construct proper URL
            if 'v.redd.it' in url and not url.endswith('.mp4'):
                # Reddit video URL format
                post_id = data.get('id', '')
                if post_id:
                    return f"https://v.redd.it/{post_id}/DASH_720.mp4"
            return url

        # Crosspost video
        if 'crosspost_parent_list' in data and data['crosspost_parent_list']:
            parent = data['crosspost_parent_list'][0]
            if 'media' in parent and parent['media']:
                reddit_video = parent['media'].get('reddit_video', {})
                if 'fallback_url' in reddit_video:
                    return reddit_video['fallback_url']

        return None

    except:
        return None

def download_video_worker(video_data):
    """Worker function for parallel downloads"""
    video_url, output_dir, video_id = video_data

    output_template = str(output_dir / f"{video_id}.%(ext)s")

    # Special handling for Reddit videos (v.redd.it)
    cmd = [
        'yt-dlp',
        video_url,
        '-f', 'bestvideo[height<=720]+bestaudio/best[height<=720]/best',
        '-o', output_template,
        '--merge-output-format', 'mp4',
        '--restrict-filenames',
        '--no-playlist',
        '--ignore-errors',
        '--no-warnings',
        '--quiet',
        '--no-check-certificate',
        '--user-agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        '--referer', 'https://www.reddit.com/',
        '--add-header', 'Accept:*/*',
    ]

    try:
        result = subprocess.run(cmd, timeout=90, capture_output=True)

        # Check if file was created
        for ext in ['.mp4', '.webm', '.mkv', '.avi']:
            if (output_dir / f"{video_id}{ext}").exists():
                with download_lock:
                    download_stats['success'] += 1
                return True

        with download_lock:
            download_stats['failed'] += 1
        return False

    except:
        with download_lock:
            download_stats['failed'] += 1
        return False

def download_from_subreddit_pushshift(subreddit_name, output_dir, max_videos=50000, workers=100):
    """Download videos from subreddit using Pushshift (UNLIMITED)"""

    subreddit_dir = output_dir / f"r_{subreddit_name}"
    subreddit_dir.mkdir(parents=True, exist_ok=True)

    print(f"\n{'='*60}")
    print(f"r/{subreddit_name} (PUSHSHIFT - NO LIMITS)")
    print('='*60)

    # Get posts from Pushshift Archive
    posts = get_reddit_posts_pushshift(subreddit_name, max_posts=max_videos * 3)

    # Extract video URLs
    video_posts = []
    for post in posts:
        video_url = extract_video_url_pushshift(post)
        if video_url:
            post_id = post.get('id', f"unknown_{len(video_posts)}")
            video_posts.append((video_url, subreddit_dir, post_id))

    print(f"  ‚úÖ Found {len(video_posts)} video posts")

    if not video_posts:
        print(f"  ‚ö†Ô∏è  No videos found")
        return 0

    # Limit to max_videos
    video_posts = video_posts[:max_videos]

    # Reset stats
    global download_stats
    download_stats = {'success': 0, 'failed': 0}

    # Download videos in parallel
    print(f"  üì• Downloading {len(video_posts)} videos with {workers} parallel workers...")

    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
        futures = [executor.submit(download_video_worker, video_data) for video_data in video_posts]

        with tqdm(total=len(video_posts), desc=f"  r/{subreddit_name}", unit="video") as pbar:
            for future in concurrent.futures.as_completed(futures):
                pbar.update(1)
                pbar.set_postfix({
                    'success': download_stats['success'],
                    'failed': download_stats['failed']
                })

    print(f"  ‚úÖ Downloaded {download_stats['success']} videos (failed: {download_stats['failed']})")
    return download_stats['success']

def main():
    import argparse

    parser = argparse.ArgumentParser(description='Reddit Pushshift Downloader (UNLIMITED)')
    parser.add_argument('--output-dir', default='/workspace/datasets/reddit_videos_pushshift',
                       help='Output directory')
    parser.add_argument('--category', choices=['fight', 'normal', 'all'], default='fight',
                       help='Category to download')
    parser.add_argument('--max-per-subreddit', type=int, default=50000,
                       help='Max videos per subreddit (Pushshift has NO limit!)')
    parser.add_argument('--workers', type=int, default=100,
                       help='Number of parallel download workers')

    args = parser.parse_args()

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    print("="*80)
    print("REDDIT PUSHSHIFT DOWNLOADER - UNLIMITED VIDEOS!")
    print("="*80)
    print(f"Output: {output_dir}")
    print(f"Category: {args.category}")
    print(f"Max per subreddit: {args.max_per_subreddit}")
    print(f"Workers: {args.workers}")
    print("")
    print("üöÄ PUSHSHIFT ADVANTAGES:")
    print("  ‚úÖ NO 1,000 post pagination limit (get 50,000+ posts!)")
    print("  ‚úÖ NO rate limiting (archive API)")
    print("  ‚úÖ Access to ALL historical Reddit posts")
    print("  ‚úÖ Same fast parallel downloading (100 workers)")
    print("")

    # Use fight subreddits
    subreddits = FIGHT_SUBREDDITS

    print("üìã SUBREDDITS TO DOWNLOAD:")
    for sub in subreddits:
        print(f"  ‚Ä¢ r/{sub['name']} (est. {sub['cctv_percent']}% CCTV)")
    print("")

    # Download from each subreddit
    total_downloaded = 0
    stats = {}
    start_time = time.time()

    for sub in subreddits:
        downloaded = download_from_subreddit_pushshift(
            sub['name'],
            output_dir,
            args.max_per_subreddit,
            args.workers
        )

        stats[sub['name']] = downloaded
        total_downloaded += downloaded

    elapsed_time = time.time() - start_time

    # Summary
    print("\n" + "="*80)
    print("DOWNLOAD COMPLETE (PUSHSHIFT)")
    print("="*80)
    print("")
    print("üìä STATISTICS BY SUBREDDIT:")
    print("-"*60)

    for sub_name, count in stats.items():
        print(f"  r/{sub_name:25s}: {count:6d} videos")

    print("-"*60)
    print(f"  {'TOTAL':25s}: {total_downloaded:6d} videos")
    print("")

    print(f"‚è±Ô∏è  TIME:")
    print(f"  Total time: {elapsed_time/60:.1f} minutes ({elapsed_time:.0f} seconds)")
    if total_downloaded > 0:
        print(f"  Average: {elapsed_time/total_downloaded:.1f} seconds per video")
    print("")

    print(f"üìÅ Location: {output_dir}")
    print("")

    # Count actual files
    all_videos = list(output_dir.rglob('*.mp4')) + list(output_dir.rglob('*.webm')) + \
                 list(output_dir.rglob('*.mkv')) + list(output_dir.rglob('*.avi'))

    print(f"‚úÖ Verified on disk: {len(all_videos)} video files")
    print("")

    if total_downloaded >= 50000:
        print("üéâ EXCELLENT: 50,000+ videos using Pushshift!")
    elif total_downloaded >= 10000:
        print("‚úÖ GREAT: 10,000+ videos downloaded")
    else:
        print(f"Downloaded {total_downloaded} videos")

    print("")

if __name__ == "__main__":
    main()
