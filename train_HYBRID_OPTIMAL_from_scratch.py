#!/usr/bin/env python3
"""
HYBRID OPTIMAL Violence Detection Training - FROM SCRATCH
Extracts VGG19 features from videos and trains with optimal configuration

Dataset location: /workspace/organized_dataset/
  ‚îú‚îÄ‚îÄ train/
  ‚îÇ   ‚îú‚îÄ‚îÄ violent/
  ‚îÇ   ‚îî‚îÄ‚îÄ nonviolent/
  ‚îú‚îÄ‚îÄ val/
  ‚îÇ   ‚îú‚îÄ‚îÄ violent/
  ‚îÇ   ‚îî‚îÄ‚îÄ nonviolent/
  ‚îî‚îÄ‚îÄ test/
      ‚îú‚îÄ‚îÄ violent/
      ‚îî‚îÄ‚îÄ nonviolent/

ROOT CAUSE OF 54.68% FAILURE:
- 50-60% dropout ‚Üí Destroyed violent pattern learning
- 10x augmentation ‚Üí 90% noise, 10% clean signal
- No per-class monitoring ‚Üí Silent bias toward "predict non-violent"
Result: 22.97% violent detection (catastrophic failure)

HYBRID SOLUTION (9 Best Features):
1. ‚úÖ MODERATE dropout (30-35%)
2. ‚úÖ BALANCED augmentation (3x)
3. ‚úÖ Per-class monitoring
4. ‚úÖ Residual connections
5. ‚úÖ Attention mechanism
6. ‚úÖ Feature compression
7. ‚úÖ Enhanced focal loss (Œ≥=3.0)
8. ‚úÖ Warmup + cosine LR
9. ‚úÖ Mixed precision FP16

EXPECTED RESULTS:
- TTA Accuracy: 90-92% (+37% from failed 54.68%)
- Violent Detection: 88-91% (+68% from failed 22.97%)
- Non-violent Detection: 92-94%
- Class Gap: <8%
"""

import os
os.environ['CUDA_VISIBLE_DEVICES'] = '1'  # Use GPU:1 for training
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, regularizers, callbacks
import cv2
import time
from pathlib import Path
import json
from datetime import datetime
from tqdm import tqdm

print("="*80)
print("üöÄ HYBRID OPTIMAL VIOLENCE DETECTION TRAINING - FROM SCRATCH")
print("="*80)
print(f"TensorFlow: {tf.__version__}")
print(f"GPU Available: {tf.config.list_physical_devices('GPU')}")

# GPU setup
gpus = tf.config.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)

# Mixed precision for speed
tf.keras.mixed_precision.set_global_policy('mixed_float16')
print("Mixed precision: FP16 enabled (2-3x speed boost)")
print("="*80 + "\n")

# ============================================================================
# OPTIMAL CONFIGURATION
# ============================================================================

CONFIG = {
    # Paths
    'dataset_path': '/workspace/organized_dataset',
    'cache_dir': '/workspace/hybrid_optimal_cache',
    'checkpoint_dir': '/workspace/hybrid_optimal_checkpoints',

    # VGG19 extraction
    'num_frames': 20,  # Extract 20 evenly-spaced frames per video
    'frame_size': (224, 224),  # VGG19 input size

    # Architecture - OPTIMAL MODERATE SIZE
    'feature_dim': 512,  # Compressed from 4096
    'lstm_units': 96,    # BALANCED
    'dropout_rate': 0.32,  # MODERATE (30-35%)
    'recurrent_dropout': 0.18,  # MODERATE (15-20%)
    'l2_reg': 0.003,  # LIGHT

    # Training
    'batch_size': 64,
    'epochs': 150,
    'early_stopping_patience': 30,

    # Augmentation - BALANCED (3x)
    'augmentation_multiplier': 3,
    'brightness_range': 0.12,
    'noise_std': 0.008,
    'temporal_jitter': True,

    # Loss
    'focal_gamma': 3.0,
    'focal_alpha': 0.5,

    # Learning rate
    'initial_lr': 0.001,
    'warmup_epochs': 5,
}

print("üìä CONFIGURATION:")
print(f"  Dataset: {CONFIG['dataset_path']}")
print(f"  Cache: {CONFIG['cache_dir']}")
print(f"  LSTM units: {CONFIG['lstm_units']}")
print(f"  Dropout: {CONFIG['dropout_rate']*100:.0f}% (MODERATE)")
print(f"  Augmentation: {CONFIG['augmentation_multiplier']}x (BALANCED)")
print(f"  Focal gamma: {CONFIG['focal_gamma']}")
print(f"  Epochs: {CONFIG['epochs']}")
print("="*80 + "\n")

# ============================================================================
# VGG19 FEATURE EXTRACTION FROM VIDEOS
# ============================================================================

def extract_vgg19_features_from_videos(split, config):
    """
    Extract VGG19 features from raw videos

    Args:
        split: 'train', 'val', or 'test'
        config: Configuration dict

    Returns:
        features_array: (N, 20, 4096) numpy array
        labels_array: (N,) numpy array
    """

    cache_file = Path(config['cache_dir']) / f'{split}_features_base.npy'
    labels_file = Path(config['cache_dir']) / f'{split}_labels_base.npy'

    # Check if already cached
    if cache_file.exists() and labels_file.exists():
        print(f"  ‚úÖ Loading cached {split} features from {cache_file}")
        features = np.load(cache_file, mmap_mode='r')
        labels = np.load(labels_file)
        print(f"     Shape: {features.shape}")
        return features, labels

    print(f"\n  üé¨ Extracting VGG19 features from {split} videos...")

    # Load VGG19
    print(f"     Loading VGG19 model...")
    base_model = tf.keras.applications.VGG19(include_top=True, weights='imagenet')
    feature_extractor = tf.keras.Model(
        inputs=base_model.input,
        outputs=base_model.get_layer('fc2').output  # 4096-dim features
    )
    print(f"     ‚úÖ VGG19 loaded (fc2 layer: 4096 dimensions)")

    # Get video paths
    split_path = Path(config['dataset_path']) / split
    violent_videos = sorted((split_path / 'violent').glob('*.mp4'))
    nonviolent_videos = sorted((split_path / 'nonviolent').glob('*.mp4'))

    print(f"     Found {len(violent_videos)} violent videos")
    print(f"     Found {len(nonviolent_videos)} non-violent videos")

    all_features = []
    all_labels = []
    failed_videos = []

    # Process all videos
    all_videos = [(v, 1) for v in violent_videos] + [(v, 0) for v in nonviolent_videos]

    print(f"     Extracting features from {len(all_videos)} videos...")

    for video_path, label in tqdm(all_videos, desc=f"     {split}"):
        try:
            # Open video
            cap = cv2.VideoCapture(str(video_path))
            if not cap.isOpened():
                failed_videos.append((str(video_path), "Cannot open"))
                continue

            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

            if total_frames < config['num_frames']:
                cap.release()
                failed_videos.append((str(video_path), f"Only {total_frames} frames"))
                continue

            # Extract evenly-spaced frames
            indices = np.linspace(0, total_frames - 1, config['num_frames'], dtype=int)
            frames = []

            for idx in indices:
                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
                ret, frame = cap.read()

                if not ret:
                    break

                # Preprocess for VGG19
                frame = cv2.resize(frame, config['frame_size'])
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frame = frame.astype(np.float32)

                # VGG19 preprocessing
                frame = tf.keras.applications.vgg19.preprocess_input(frame)
                frames.append(frame)

            cap.release()

            if len(frames) < config['num_frames']:
                failed_videos.append((str(video_path), f"Only extracted {len(frames)} frames"))
                continue

            # Extract features using VGG19
            frames_batch = np.array(frames)
            features = feature_extractor.predict(frames_batch, verbose=0)

            # Normalize features
            features = features / (np.linalg.norm(features, axis=-1, keepdims=True) + 1e-8)

            all_features.append(features)
            all_labels.append(label)

        except Exception as e:
            failed_videos.append((str(video_path), f"Error: {e}"))
            continue

    if failed_videos:
        print(f"\n     ‚ö†Ô∏è  Failed to process {len(failed_videos)} videos:")
        for video, reason in failed_videos[:5]:  # Show first 5
            print(f"        {Path(video).name}: {reason}")
        if len(failed_videos) > 5:
            print(f"        ... and {len(failed_videos) - 5} more")

    # Convert to arrays
    features_array = np.array(all_features, dtype=np.float32)
    labels_array = np.array(all_labels, dtype=np.int32)

    print(f"\n     ‚úÖ Extracted features: {features_array.shape}")
    print(f"        Violent: {labels_array.sum()}")
    print(f"        Non-violent: {len(labels_array) - labels_array.sum()}")

    # Save cache
    Path(config['cache_dir']).mkdir(parents=True, exist_ok=True)
    np.save(cache_file, features_array)
    np.save(labels_file, labels_array)
    print(f"     üíæ Cached to: {cache_file}")

    return features_array, labels_array

# ============================================================================
# BALANCED AUGMENTATION (3x)
# ============================================================================

def apply_balanced_augmentation(features, config):
    """
    3x augmentation: 33% clean, 67% augmented
    Preserves violent patterns vs failed 10x (10% clean, 90% noise)
    """
    augmented = [features]  # Original

    # Aug 1: Slight brightness
    brightness_factor = 1.0 + np.random.uniform(-config['brightness_range'], config['brightness_range'])
    aug1 = features * brightness_factor
    aug1 = np.clip(aug1, features.min(), features.max())
    augmented.append(aug1)

    # Aug 2: Temporal jittering + light noise
    if config['temporal_jitter']:
        aug2 = features.copy()
        num_frames = features.shape[0]
        for i in range(0, num_frames, 4):
            end = min(i + 4, num_frames)
            if end - i > 1 and np.random.random() > 0.5:
                indices = np.arange(i, end)
                np.random.shuffle(indices)
                aug2[i:end] = aug2[indices]

        # Add light noise
        noise = np.random.normal(0, config['noise_std'], aug2.shape)
        aug2 = aug2 + noise
        augmented.append(aug2)
    else:
        noise = np.random.normal(0, config['noise_std'], features.shape)
        aug2 = features + noise
        augmented.append(aug2)

    return np.array(augmented)

def augment_dataset(features, labels, config):
    """Apply 3x augmentation to training set"""
    print(f"\n  üîÑ Applying {config['augmentation_multiplier']}x augmentation...")

    all_features = []
    all_labels = []

    for feat, label in tqdm(zip(features, labels), total=len(features), desc="     Augmenting"):
        augmented = apply_balanced_augmentation(feat, config)
        for aug_feat in augmented:
            all_features.append(aug_feat)
            all_labels.append(label)

    features_array = np.array(all_features, dtype=np.float32)
    labels_array = np.array(all_labels, dtype=np.int32)

    print(f"     ‚úÖ Augmented: {len(features)} ‚Üí {len(features_array)} samples")
    print(f"        Violent: {labels_array.sum()}")
    print(f"        Non-violent: {len(labels_array) - labels_array.sum()}")

    return features_array, labels_array

# ============================================================================
# PER-CLASS ACCURACY MONITORING
# ============================================================================

class PerClassAccuracyCallback(callbacks.Callback):
    """Monitor violent/non-violent accuracy separately - CRITICAL for detecting bias"""

    def __init__(self, validation_data):
        super().__init__()
        self.validation_data = validation_data
        self.history = {
            'violent_accuracy': [],
            'nonviolent_accuracy': [],
            'accuracy_gap': [],
            'epoch': []
        }

    def on_epoch_end(self, epoch, logs=None):
        X_val, y_val = self.validation_data

        # Predict
        y_pred = self.model.predict(X_val, verbose=0)
        y_pred_classes = (y_pred[:, 1] > 0.5).astype(int)
        y_true_classes = y_val[:, 1].astype(int)

        # Per-class accuracy
        violent_mask = y_true_classes == 1
        nonviolent_mask = y_true_classes == 0

        violent_acc = np.mean(y_pred_classes[violent_mask] == y_true_classes[violent_mask]) if violent_mask.sum() > 0 else 0
        nonviolent_acc = np.mean(y_pred_classes[nonviolent_mask] == y_true_classes[nonviolent_mask]) if nonviolent_mask.sum() > 0 else 0
        gap = abs(violent_acc - nonviolent_acc)

        # Store
        self.history['violent_accuracy'].append(violent_acc)
        self.history['nonviolent_accuracy'].append(nonviolent_acc)
        self.history['accuracy_gap'].append(gap)
        self.history['epoch'].append(epoch + 1)

        # Status
        if gap < 0.08:
            status = "‚úÖ EXCELLENT"
        elif gap < 0.15:
            status = "‚úÖ GOOD"
        elif gap < 0.25:
            status = "‚ö†Ô∏è  WARNING"
        else:
            status = "üö® CRITICAL"

        # Log
        print(f"\n  üìä Per-Class Accuracy (Epoch {epoch+1}):")
        print(f"    Violent:     {violent_acc*100:5.2f}%")
        print(f"    Non-violent: {nonviolent_acc*100:5.2f}%")
        print(f"    Gap:         {gap*100:5.2f}% {status}")

        if gap > 0.25:
            print(f"    üö® CRITICAL: Gap exceeds 25% - model is biased!")
        elif gap > 0.15:
            print(f"    ‚ö†Ô∏è  WARNING: Gap exceeds 15% - monitor closely")

        logs['violent_accuracy'] = violent_acc
        logs['nonviolent_accuracy'] = nonviolent_acc
        logs['accuracy_gap'] = gap

# ============================================================================
# ENHANCED FOCAL LOSS
# ============================================================================

class EnhancedFocalLoss(tf.keras.losses.Loss):
    """Focal Loss with gamma=3.0 - forces hard example learning"""

    def __init__(self, gamma=3.0, alpha=0.5, **kwargs):
        super().__init__(**kwargs)
        self.gamma = gamma
        self.alpha = alpha

    def call(self, y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.cast(y_pred, tf.float32)

        epsilon = tf.keras.backend.epsilon()
        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)

        pt = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)
        focal_weight = tf.pow(1 - pt, self.gamma)

        bce = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)
        focal_loss = self.alpha * focal_weight * bce

        return tf.reduce_mean(focal_loss)

# ============================================================================
# HYBRID OPTIMAL ARCHITECTURE
# ============================================================================

def build_hybrid_optimal_model(input_shape, config):
    """
    HYBRID architecture:
    - Feature compression (4096 ‚Üí 512)
    - Residual connections
    - Attention mechanism
    - Moderate dropout (32%)
    Expected: ~1.2M parameters, 90-92% TTA accuracy
    """

    inputs = layers.Input(shape=input_shape, name='input_features')

    # Feature compression
    x = layers.Dense(config['feature_dim'], activation='relu', name='feature_compression')(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(config['dropout_rate'] * 0.5)(x)

    # BiLSTM Layer 1
    x = layers.Bidirectional(
        layers.LSTM(
            config['lstm_units'],
            return_sequences=True,
            dropout=config['dropout_rate'],
            recurrent_dropout=config['recurrent_dropout'],
            kernel_regularizer=regularizers.l2(config['l2_reg'])
        ),
        name='bilstm_1'
    )(x)
    x = layers.BatchNormalization()(x)

    # Residual connection
    x_residual = x

    # BiLSTM Layer 2
    x = layers.Bidirectional(
        layers.LSTM(
            config['lstm_units'],
            return_sequences=True,
            dropout=config['dropout_rate'],
            recurrent_dropout=config['recurrent_dropout'],
            kernel_regularizer=regularizers.l2(config['l2_reg'])
        ),
        name='bilstm_2'
    )(x)
    x = layers.BatchNormalization()(x)

    # Add residual
    x = layers.Add(name='residual_add')([x, x_residual])

    # BiLSTM Layer 3
    x = layers.Bidirectional(
        layers.LSTM(
            config['lstm_units'] // 2,
            return_sequences=True,
            dropout=config['dropout_rate'],
            recurrent_dropout=config['recurrent_dropout'],
            kernel_regularizer=regularizers.l2(config['l2_reg'])
        ),
        name='bilstm_3'
    )(x)
    x = layers.BatchNormalization()(x)

    # Attention mechanism
    attention_score = layers.Dense(1, activation='tanh', name='attention_score')(x)
    attention_score = layers.Flatten()(attention_score)
    attention_weights = layers.Activation('softmax', name='attention_weights')(attention_score)
    attention_weights_expanded = layers.RepeatVector(config['lstm_units'])(attention_weights)
    attention_weights_expanded = layers.Permute([2, 1])(attention_weights_expanded)

    attended = layers.Multiply(name='attended_features')([x, attention_weights_expanded])
    attended = layers.Lambda(lambda t: tf.reduce_sum(t, axis=1), name='attention_pooling')(attended)

    # Dense layers
    x = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(config['l2_reg']), name='dense_1')(attended)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(config['dropout_rate'])(x)

    x = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(config['l2_reg']), name='dense_2')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(config['dropout_rate'] * 0.8)(x)

    # Output
    outputs = layers.Dense(2, activation='softmax', dtype='float32', name='output')(x)

    model = models.Model(inputs=inputs, outputs=outputs, name='HybridOptimalViolenceDetector')

    return model

# ============================================================================
# LEARNING RATE SCHEDULE
# ============================================================================

def create_lr_schedule(config):
    """Warmup + Cosine decay"""
    def lr_schedule(epoch):
        if epoch < config['warmup_epochs']:
            return config['initial_lr'] * (epoch + 1) / config['warmup_epochs']
        else:
            decay_epochs = config['epochs'] - config['warmup_epochs']
            epoch_in_decay = epoch - config['warmup_epochs']
            cosine_decay = 0.5 * (1 + np.cos(np.pi * epoch_in_decay / decay_epochs))
            return config['initial_lr'] * cosine_decay + 1e-7
    return lr_schedule

# ============================================================================
# MIXED PRECISION METRICS
# ============================================================================

class BinaryAccuracy(tf.keras.metrics.Metric):
    """Mixed precision compatible binary accuracy"""

    def __init__(self, **kwargs):
        super().__init__(name='binary_accuracy', dtype=tf.float32, **kwargs)
        self.correct = self.add_weight(name='correct', initializer='zeros', dtype=tf.float32)
        self.total = self.add_weight(name='total', initializer='zeros', dtype=tf.float32)

    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true = tf.cast(tf.reshape(y_true, [-1]), tf.int32)
        y_pred_class = tf.cast(tf.argmax(y_pred, axis=-1), tf.int32)
        matches = tf.cast(tf.equal(y_true, y_pred_class), tf.float32)
        self.correct.assign_add(tf.reduce_sum(matches))
        self.total.assign_add(tf.cast(tf.size(y_true), tf.float32))

    def result(self):
        return tf.divide(self.correct, self.total + tf.keras.backend.epsilon())

    def reset_state(self):
        self.correct.assign(0.0)
        self.total.assign(0.0)

# ============================================================================
# MAIN TRAINING
# ============================================================================

def main():
    print("\n" + "="*80)
    print("üì• STEP 1: EXTRACTING VGG19 FEATURES FROM VIDEOS")
    print("="*80)

    extraction_start = time.time()

    # Extract features from videos
    X_train_base, y_train_base = extract_vgg19_features_from_videos('train', CONFIG)
    X_val, y_val = extract_vgg19_features_from_videos('val', CONFIG)

    extraction_time = time.time() - extraction_start
    print(f"\n‚è±Ô∏è  Feature extraction time: {extraction_time/3600:.1f} hours")

    print("\n" + "="*80)
    print("üì• STEP 2: APPLYING BALANCED AUGMENTATION")
    print("="*80)

    # Apply 3x augmentation to training set only
    X_train, y_train = augment_dataset(X_train_base, y_train_base, CONFIG)

    print(f"\nüìä Final Dataset Statistics:")
    print(f"  Train: {X_train.shape} | Violent: {y_train.sum():,} | Non-violent: {len(y_train) - y_train.sum():,}")
    print(f"  Val:   {X_val.shape} | Violent: {y_val.sum():,} | Non-violent: {len(y_val) - y_val.sum():,}")

    # Convert to categorical
    y_train_cat = tf.keras.utils.to_categorical(y_train, 2)
    y_val_cat = tf.keras.utils.to_categorical(y_val, 2)

    # Create datasets
    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train_cat))
    train_dataset = train_dataset.shuffle(20000).batch(CONFIG['batch_size']).prefetch(tf.data.AUTOTUNE)

    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val_cat))
    val_dataset = val_dataset.batch(CONFIG['batch_size']).prefetch(tf.data.AUTOTUNE)

    print("\n" + "="*80)
    print("üèóÔ∏è  STEP 3: BUILDING HYBRID OPTIMAL MODEL")
    print("="*80)

    model = build_hybrid_optimal_model((20, 4096), CONFIG)
    model.summary()

    print(f"\nüìä Model Statistics:")
    print(f"  Total parameters: {model.count_params():,}")
    print(f"  Architecture: Hybrid (residual + attention + compression)")

    # Compile
    lr_schedule = create_lr_schedule(CONFIG)
    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule(0), clipnorm=1.0)

    model.compile(
        optimizer=optimizer,
        loss=EnhancedFocalLoss(gamma=CONFIG['focal_gamma'], alpha=CONFIG['focal_alpha']),
        metrics=[BinaryAccuracy()]
    )

    print("\n" + "="*80)
    print("üöÄ STEP 4: TRAINING WITH OPTIMAL CONFIGURATION")
    print("="*80)

    # Callbacks
    checkpoint_path = Path(CONFIG['checkpoint_dir'])
    checkpoint_path.mkdir(parents=True, exist_ok=True)

    callbacks_list = [
        PerClassAccuracyCallback((X_val, y_val_cat)),

        callbacks.EarlyStopping(
            monitor='val_binary_accuracy',
            patience=CONFIG['early_stopping_patience'],
            restore_best_weights=True,
            mode='max',
            verbose=1
        ),

        callbacks.ModelCheckpoint(
            str(checkpoint_path / 'hybrid_best_{epoch:03d}_{val_binary_accuracy:.4f}.h5'),
            monitor='val_binary_accuracy',
            save_best_only=True,
            mode='max',
            verbose=1
        ),

        callbacks.LearningRateScheduler(lr_schedule, verbose=1),

        callbacks.ReduceLROnPlateau(
            monitor='val_binary_accuracy',
            factor=0.5,
            patience=15,
            min_lr=1e-7,
            mode='max',
            verbose=1
        ),

        callbacks.CSVLogger(str(checkpoint_path / 'training_history.csv'), append=False),
        callbacks.TensorBoard(log_dir=str(checkpoint_path / 'tensorboard'), histogram_freq=1)
    ]

    print("\nüî• TRAINING FEATURES ACTIVE:")
    print(f"  ‚úÖ Moderate dropout: {CONFIG['dropout_rate']*100:.0f}% (preserves patterns)")
    print(f"  ‚úÖ Balanced augmentation: {CONFIG['augmentation_multiplier']}x (not excessive)")
    print(f"  ‚úÖ Per-class monitoring (catches bias early)")
    print(f"  ‚úÖ Residual connections (better gradients)")
    print(f"  ‚úÖ Attention mechanism (focuses on violence)")
    print(f"  ‚úÖ Feature compression (efficiency)")
    print(f"  ‚úÖ Enhanced focal loss Œ≥={CONFIG['focal_gamma']} (hard mining)")
    print(f"  ‚úÖ Warmup + cosine LR schedule")
    print(f"  ‚úÖ Mixed precision FP16 (speed)")
    print(f"  ‚úÖ Gradient clipping (stability)")
    print()

    # Train
    training_start = time.time()

    history = model.fit(
        train_dataset,
        validation_data=val_dataset,
        epochs=CONFIG['epochs'],
        callbacks=callbacks_list,
        verbose=1
    )

    training_time = time.time() - training_start
    total_time = time.time() - extraction_start

    print("\n" + "="*80)
    print("‚úÖ TRAINING COMPLETE")
    print("="*80)
    print(f"‚è±Ô∏è  Feature extraction: {extraction_time/3600:.1f} hours")
    print(f"‚è±Ô∏è  Training time: {training_time/3600:.1f} hours")
    print(f"‚è±Ô∏è  Total time: {total_time/3600:.1f} hours")
    print(f"üìä Best val accuracy: {max(history.history['val_binary_accuracy'])*100:.2f}%")

    # Per-class results
    per_class_cb = [cb for cb in callbacks_list if isinstance(cb, PerClassAccuracyCallback)][0]
    final_violent = per_class_cb.history['violent_accuracy'][-1]
    final_nonviolent = per_class_cb.history['nonviolent_accuracy'][-1]
    final_gap = per_class_cb.history['accuracy_gap'][-1]

    print(f"\nüéØ FINAL PER-CLASS PERFORMANCE:")
    print(f"   Violent:     {final_violent*100:.2f}%")
    print(f"   Non-violent: {final_nonviolent*100:.2f}%")
    print(f"   Gap:         {final_gap*100:.2f}%")

    if final_violent > 0.88 and final_nonviolent > 0.88 and final_gap < 0.08:
        print(f"\nüéâ SUCCESS! Both classes performing excellently!")
        print(f"   Expected TTA accuracy: 90-92%")
    elif final_violent > 0.85 and final_gap < 0.12:
        print(f"\n‚úÖ GOOD! Model is balanced and performing well")
        print(f"   Expected TTA accuracy: 88-90%")
    else:
        print(f"\n‚ö†Ô∏è  Performance below expectations")

    # Save results
    results = {
        'timestamp': datetime.now().isoformat(),
        'architecture': 'hybrid_optimal',
        'extraction_time_hours': extraction_time / 3600,
        'training_time_hours': training_time / 3600,
        'total_time_hours': total_time / 3600,
        'best_val_accuracy': float(max(history.history['val_binary_accuracy'])),
        'final_violent_accuracy': float(final_violent),
        'final_nonviolent_accuracy': float(final_nonviolent),
        'final_accuracy_gap': float(final_gap),
        'total_parameters': int(model.count_params()),
    }

    with open(checkpoint_path / 'training_results.json', 'w') as f:
        json.dump(results, f, indent=2)

    print(f"\nüíæ Checkpoints: {checkpoint_path}")
    print(f"üíæ Results: {checkpoint_path / 'training_results.json'}")

    print("\n" + "="*80)
    print("üéØ NEXT STEPS:")
    print("="*80)
    print("1. Test with TTA: python3 predict_with_tta_simple.py")
    print("2. Expected TTA accuracy: 90-92%")
    print("3. Deploy if TTA > 88%")
    print("="*80)

    return model, history

if __name__ == "__main__":
    model, history = main()
