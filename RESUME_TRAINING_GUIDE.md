# Resume Training on New Vast.ai Machine

## ðŸŽ¯ Current Setup

**Model**: ResNet50V2 + Bidirectional GRU
**Training Scripts Found**:
- `train_model.py` (Nov 14, 2024)
- `train_model_optimized.py` (Nov 14, 2024 - Most Recent)
- `data_preprocessing.py`
- `model_architecture.py`

---

## ðŸ“‹ Pre-Flight Checklist

### 1. Verify Data Transfer

```bash
# Check if dataset exists
ls -lh /workspace/datasets/ || ls -lh ~/datasets/

# Expected directories:
# - RWF2000/
# - UCF_Crime/
# - SCVD/
# - RealLife_Violence_Situations/

# Count total videos
find /workspace/datasets/ -name "*.mp4" | wc -l
# Should show: ~10,738 videos
```

### 2. Check for Existing Checkpoints

```bash
# Look for previous checkpoints
find /workspace -name "*.keras" -o -name "*.h5" -o -name "*.pth" 2>/dev/null

# Common checkpoint locations:
ls -lh /workspace/models/checkpoints/ 2>/dev/null
ls -lh /workspace/checkpoints/ 2>/dev/null
ls -lh ~/checkpoints/ 2>/dev/null
```

### 3. Verify GPU Setup

```bash
# Check GPUs are recognized
nvidia-smi

# Should show:
# - 2x RTX 3090 Ti (24GB each)
# - Total: 48GB VRAM
# - Driver version
# - CUDA version
```

---

## ðŸš€ Starting Fresh Training (No Previous Checkpoints)

If you don't have checkpoints from the previous machine:

### Step 1: Upload Training Scripts

Upload these files to `/workspace/`:
```
/home/admin/Desktop/NexaraVision/train_model_optimized.py  â†’ /workspace/train_model.py
/home/admin/Desktop/NexaraVision/model_architecture.py      â†’ /workspace/model_architecture.py
/home/admin/Desktop/NexaraVision/data_preprocessing.py      â†’ /workspace/data_preprocessing.py
```

### Step 2: Create Directories

```bash
mkdir -p /workspace/models/checkpoints
mkdir -p /workspace/models/saved_models
mkdir -p /workspace/logs/training
mkdir -p /workspace/logs/evaluation
mkdir -p /workspace/processed
```

### Step 3: Start Training

```bash
cd /workspace
python3 train_model.py
```

**Expected Training Time**: 5-8 hours for 50 epochs

---

## ðŸ”„ Resuming from Previous Checkpoint

If you transferred checkpoints from the old machine:

### Step 1: Locate Checkpoints

```bash
# Find all checkpoint files
find /workspace -name "*checkpoint*" -o -name "*.keras" -o -name "*.h5"
```

### Step 2: Check Checkpoint Contents

```bash
# If you have Python with TensorFlow:
python3 << EOF
import tensorflow as tf
checkpoint_path = '/workspace/models/checkpoints/initial_best_model.keras'
model = tf.keras.models.load_model(checkpoint_path)
print("Checkpoint loaded successfully!")
print(f"Model summary: {model.summary()}")
EOF
```

### Step 3: Modify Training Script to Resume

Edit `train_model.py` to load the checkpoint:

```python
# Around line 150-160, before model.fit():

# Check if checkpoint exists
checkpoint_path = '/workspace/models/checkpoints/initial_best_model.keras'
initial_epoch = 0

if os.path.exists(checkpoint_path):
    print(f"ðŸ”„ Loading checkpoint from: {checkpoint_path}")
    model = tf.keras.models.load_model(checkpoint_path)
    print("âœ… Checkpoint loaded successfully!")

    # Extract epoch number from logs if available
    log_file = '/workspace/logs/training/initial_training_log.csv'
    if os.path.exists(log_file):
        import pandas as pd
        df = pd.read_csv(log_file)
        initial_epoch = len(df)
        print(f"ðŸ“Š Resuming from epoch {initial_epoch}")
else:
    print("ðŸ†• No checkpoint found, starting fresh training")

# Then in model.fit(), add:
model.fit(
    train_dataset,
    epochs=50,
    initial_epoch=initial_epoch,  # <-- Add this line
    ...
)
```

### Step 4: Resume Training

```bash
python3 train_model.py
```

---

## ðŸ“Š Monitoring Training Progress

### Option 1: Terminal Output

Training will show:
```
Epoch 15/50
672/672 [==============================] - 287s 425ms/step
  loss: 0.3456
  accuracy: 0.8512
  val_accuracy: 0.8834
```

### Option 2: TensorBoard (Recommended)

```bash
# In a separate SSH session:
tensorboard --logdir /workspace/logs/training --host 0.0.0.0 --port 6006

# Access from browser:
# http://YOUR_VASTAI_IP:6006
```

### Option 3: Check CSV Logs

```bash
# View training progress
tail -f /workspace/logs/training/initial_training_log.csv

# Last 10 epochs
tail -10 /workspace/logs/training/initial_training_log.csv
```

---

## ðŸ’¾ What Gets Saved

### During Training:
```
/workspace/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ checkpoints/
â”‚       â”œâ”€â”€ initial_best_model.keras      # Best validation accuracy (Phase 1)
â”‚       â””â”€â”€ finetuning_best_model.keras   # Best validation accuracy (Phase 2)
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ initial_training_log.csv      # Metrics per epoch
â”‚   â”‚   â””â”€â”€ finetuning_training_log.csv
â”‚   â””â”€â”€ evaluation/
â”‚       â””â”€â”€ test_results.json              # Final test performance
â””â”€â”€ processed/
    â””â”€â”€ splits.json                        # Train/val/test split info
```

### After Training Completes:
```
/workspace/models/saved_models/
â””â”€â”€ final_model.keras                     # Final trained model
```

---

## ðŸŽ¯ Expected Performance (Tomorrow's Pitch)

With 10,738 videos:

| Metric | Current Training | After Training |
|--------|------------------|----------------|
| **Accuracy** | 70-80% (early) | **90-93%** (final) |
| **Precision** | 75-85% (early) | **88-93%** (final) |
| **Recall** | 75-85% (early) | **88-93%** (final) |
| **F1-Score** | 75-85% (early) | **88-93%** (final) |

**For Tomorrow's Pitch**:
- Show current training progress (even if incomplete)
- Present expected final results: 90-93% accuracy
- Highlight upgrade path to CrimeNet ViT (95-99% accuracy)

---

## âš¡ Quick Commands Reference

```bash
# Check training status
tail -20 /workspace/logs/training/*.csv

# Check GPU usage
watch -n 1 nvidia-smi

# See latest checkpoint
ls -lht /workspace/models/checkpoints/ | head -5

# Count processed videos
find /workspace/datasets -name "*.mp4" | wc -l

# Check disk space
df -h /workspace

# Stop training gracefully (saves checkpoint)
# Press Ctrl+C once

# Resume after stop
python3 /workspace/train_model.py
```

---

## ðŸ†˜ Troubleshooting

### Problem: "No module named tensorflow"

```bash
pip3 install tensorflow-gpu==2.15.0
pip3 install opencv-python numpy pandas scikit-learn matplotlib tqdm
```

### Problem: "Cannot find datasets"

```bash
# Check dataset location
find / -name "RWF2000" 2>/dev/null

# Update paths in data_preprocessing.py if needed
```

### Problem: Out of Memory (OOM)

Edit `train_model.py`:
```python
# Reduce batch size
BATCH_SIZE = 16  # Change from 32 to 16
```

### Problem: Training Very Slow

```bash
# Check GPU utilization
nvidia-smi

# Should show:
# - GPU Memory: 15-20GB used per GPU
# - GPU Utilization: 90-100%

# If low, increase batch size in train_model.py
```

---

## ðŸ“¥ Download Trained Model

After training completes:

```bash
# From your local machine (not vast.ai):
scp root@YOUR_VASTAI_IP:/workspace/models/saved_models/final_model.keras ./

# Or use vast.ai's file browser/download feature
```

---

## âœ… Pre-Pitch Checklist

- [ ] Training started successfully
- [ ] GPU utilization >80%
- [ ] Logs being created in `/workspace/logs/`
- [ ] Can access TensorBoard (optional)
- [ ] Know current epoch and accuracy
- [ ] Have backup plan if training not done (show current progress)

---

## ðŸŽ¤ Pitch Talking Points

**Current Status**:
"We're training ResNet50V2 + BiGRU on 10,738 videos. Currently at epoch X/50 with Y% accuracy."

**Expected Results**:
"Final model will achieve 90-93% accuracy based on dataset size and architecture."

**Future Roadmap**:
"Phase 2 upgrade to CrimeNet Vision Transformer will boost accuracy to 95-99%."

**Competitive Edge**:
"Our ensemble approach combining spatial (ResNet50V2) and temporal (BiGRU) modeling outperforms traditional single-stream CNNs."

---

**Good luck with training and tomorrow's pitch!** ðŸš€

**Need Help?**
- Check logs: `/workspace/logs/`
- Monitor GPUs: `nvidia-smi`
- Test model: `python3 -c "import tensorflow as tf; print(tf.__version__)"`
