{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Violence Detection MVP - End-to-End Demo\n",
    "\n",
    "This notebook demonstrates the complete Violence Detection MVP pipeline, from data preprocessing to model training, evaluation, and inference.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The Violence Detection MVP uses:\n",
    "- **VGG19** for feature extraction from video frames\n",
    "- **LSTM with Attention** for sequence classification\n",
    "- **Transfer Learning** approach for efficient training\n",
    "\n",
    "## Pipeline Steps\n",
    "\n",
    "1. **Data Preprocessing**: Extract frames from videos and create labels\n",
    "2. **Feature Extraction**: Use VGG19 to extract features from frames\n",
    "3. **Model Training**: Train LSTM-Attention model on extracted features\n",
    "4. **Evaluation**: Evaluate model performance with comprehensive metrics\n",
    "5. **Inference**: Make predictions on new videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add src directory to path\n",
    "project_root = Path().absolute().parent\n",
    "src_path = project_root / 'src'\n",
    "sys.path.append(str(src_path))\n",
    "\n",
    "# Import project modules\n",
    "from config import Config\n",
    "from data_preprocessing import DataPreprocessor, VideoFrameExtractor\n",
    "from feature_extraction import FeaturePipeline\n",
    "from model_architecture import ViolenceDetectionModel\n",
    "from training import TrainingPipeline, ExperimentManager\n",
    "from evaluation import ModelEvaluator\n",
    "from inference import ViolencePredictor, InferenceAPI\n",
    "from visualization import TrainingVisualizer, EvaluationVisualizer, DataVisualizer\n",
    "from utils import SystemInfo, validate_project_setup\n",
    "\n",
    "print(\"All modules imported successfully!\")\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check system information\n",
    "system_info = SystemInfo.get_system_info()\n",
    "dependencies = SystemInfo.check_dependencies()\n",
    "\n",
    "print(\"System Information:\")\n",
    "print(f\"Platform: {system_info['platform']}\")\n",
    "print(f\"Python Version: {system_info['python_version']}\")\n",
    "print(f\"Memory: {system_info['memory_total_gb']:.1f} GB total, {system_info['memory_available_gb']:.1f} GB available\")\n",
    "print(f\"CPU Count: {system_info['cpu_count']}\")\n",
    "\n",
    "print(\"\\nDependency Check:\")\n",
    "for dep, available in dependencies['dependencies'].items():\n",
    "    status = \"✓\" if available else \"✗\"\n",
    "    version = dependencies['versions'][dep]\n",
    "    print(f\"{status} {dep}: {version}\")\n",
    "\n",
    "print(f\"\\nAll dependencies available: {dependencies['all_available']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize configuration\n",
    "config = Config()\n",
    "\n",
    "print(\"Configuration Settings:\")\n",
    "print(f\"Image Size: {config.IMG_SIZE}x{config.IMG_SIZE}\")\n",
    "print(f\"Frames per Video: {config.FRAMES_PER_VIDEO}\")\n",
    "print(f\"Batch Size: {config.BATCH_SIZE}\")\n",
    "print(f\"Learning Rate: {config.LEARNING_RATE}\")\n",
    "print(f\"RNN Size: {config.RNN_SIZE}\")\n",
    "print(f\"Dropout Rate: {config.DROPOUT_RATE}\")\n",
    "\n",
    "print(f\"\\nData Directories:\")\n",
    "print(f\"Raw Data: {config.RAW_DATA_DIR}\")\n",
    "print(f\"Processed Data: {config.PROCESSED_DATA_DIR}\")\n",
    "print(f\"Models: {config.MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and validate model architecture\n",
    "from model_architecture import validate_model_architecture\n",
    "\n",
    "validation_result = validate_model_architecture(config)\n",
    "\n",
    "if validation_result['success']:\n",
    "    print(\"✓ Model architecture validation successful!\")\n",
    "    print(f\"Output shape: {validation_result['output_shape']}\")\n",
    "    print(f\"Total parameters: {validation_result['metrics']['total_parameters']:,}\")\n",
    "    print(f\"Trainable parameters: {validation_result['metrics']['trainable_parameters']:,}\")\n",
    "    print(f\"Model size: {validation_result['metrics']['model_size_mb']:.1f} MB\")\n",
    "else:\n",
    "    print(\"✗ Model architecture validation failed!\")\n",
    "    print(f\"Error: {validation_result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display model\n",
    "model_builder = ViolenceDetectionModel(config)\n",
    "model = model_builder.create_model()\n",
    "\n",
    "print(\"Model Architecture:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline Demo (if data available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if sample data is available\n",
    "sample_data_dir = config.RAW_DATA_DIR\n",
    "\n",
    "if sample_data_dir.exists() and any(sample_data_dir.iterdir()):\n",
    "    print(f\"Sample data found in: {sample_data_dir}\")\n",
    "    \n",
    "    # List available files\n",
    "    video_files = []\n",
    "    for ext in config.VIDEO_EXTENSIONS:\n",
    "        video_files.extend(list(sample_data_dir.glob(f\"*{ext}\")))\n",
    "    \n",
    "    print(f\"Found {len(video_files)} video files\")\n",
    "    \n",
    "    if video_files:\n",
    "        # Show first few files\n",
    "        print(\"\\nSample files:\")\n",
    "        for i, file_path in enumerate(video_files[:5]):\n",
    "            print(f\"  {i+1}. {file_path.name}\")\n",
    "        \n",
    "        if len(video_files) > 5:\n",
    "            print(f\"  ... and {len(video_files) - 5} more\")\n",
    "else:\n",
    "    print(f\"No sample data found in: {sample_data_dir}\")\n",
    "    print(\"To run the full demo:\")\n",
    "    print(\"1. Place video files in the data/raw/ directory\")\n",
    "    print(\"2. Use naming convention: 'fi_*.avi' or 'V_*.avi' for violence videos\")\n",
    "    print(\"3. Use naming convention: 'no_*.avi' or 'NV_*.avi' for non-violence videos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo feature extraction pipeline\n",
    "from feature_extraction import print_vgg19_info\n",
    "\n",
    "print(\"VGG19 Model Information:\")\n",
    "print_vgg19_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Demo (Synthetic Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data for demonstration\n",
    "def create_synthetic_data(num_samples=100):\n",
    "    \"\"\"Create synthetic data for demo purposes.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create synthetic features (20 frames x 4096 features)\n",
    "    data = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Random features\n",
    "        features = np.random.normal(0, 1, (config.FRAMES_PER_VIDEO, config.TRANSFER_VALUES_SIZE))\n",
    "        features = features.astype(np.float16)\n",
    "        \n",
    "        # Random label\n",
    "        if i < num_samples // 2:\n",
    "            label = [1, 0]  # Violence\n",
    "        else:\n",
    "            label = [0, 1]  # No violence\n",
    "        \n",
    "        data.append(features)\n",
    "        targets.append(label)\n",
    "    \n",
    "    return data, targets\n",
    "\n",
    "# Create synthetic data\n",
    "print(\"Creating synthetic data for demo...\")\n",
    "synthetic_data, synthetic_targets = create_synthetic_data(100)\n",
    "\n",
    "print(f\"Created {len(synthetic_data)} synthetic samples\")\n",
    "print(f\"Data shape: {synthetic_data[0].shape}\")\n",
    "print(f\"Label distribution: {np.sum([t[0] for t in synthetic_targets])} violence, {np.sum([t[1] for t in synthetic_targets])} no violence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Training Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick training demo with synthetic data\n",
    "# Split data\n",
    "split_idx = int(len(synthetic_data) * 0.8)\n",
    "train_data = synthetic_data[:split_idx]\n",
    "train_targets = synthetic_targets[:split_idx]\n",
    "test_data = synthetic_data[split_idx:]\n",
    "test_targets = synthetic_targets[split_idx:]\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "\n",
    "# Create a small model for quick demo\n",
    "demo_config = Config()\n",
    "demo_config.EPOCHS = 3  # Very few epochs for demo\n",
    "demo_config.BATCH_SIZE = 8\n",
    "demo_config.RNN_SIZE = 32  # Smaller model\n",
    "\n",
    "demo_model_builder = ViolenceDetectionModel(demo_config)\n",
    "demo_model = demo_model_builder.create_model()\n",
    "\n",
    "print(\"\\nTraining demo model (3 epochs)...\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train = np.array(train_data[:32])  # Use subset for quick demo\n",
    "y_train = np.array(train_targets[:32])\n",
    "X_test = np.array(test_data)\n",
    "y_test = np.array(test_targets)\n",
    "\n",
    "# Train\n",
    "history = demo_model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=demo_config.BATCH_SIZE,\n",
    "    epochs=demo_config.EPOCHS,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nDemo training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training results\n",
    "training_viz = TrainingVisualizer(config)\n",
    "\n",
    "# Plot training history\n",
    "fig = training_viz.plot_training_history(\n",
    "    history.history,\n",
    "    show_plot=True\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the demo model\n",
    "print(\"Evaluating demo model...\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = demo_model.predict(X_test, verbose=0)\n",
    "binary_predictions = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Calculate basic metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "accuracy = accuracy_score(y_test, binary_predictions)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, binary_predictions, target_names=['Violence', 'No Violence']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize evaluation results\n",
    "eval_viz = EvaluationVisualizer(config)\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true_single = np.argmax(y_test, axis=1)\n",
    "y_pred_single = np.argmax(binary_predictions, axis=1)\n",
    "cm = confusion_matrix(y_true_single, y_pred_single)\n",
    "\n",
    "fig = eval_viz.plot_confusion_matrix(cm, show_plot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo inference on synthetic data\n",
    "print(\"Inference Demo:\")\n",
    "\n",
    "# Use the trained model for inference\n",
    "sample_features = X_test[0:1]  # Take first test sample\n",
    "true_label = y_test[0]\n",
    "\n",
    "# Make prediction\n",
    "prediction = demo_model.predict(sample_features, verbose=0)[0]\n",
    "predicted_class_idx = np.argmax(prediction)\n",
    "confidence = prediction[predicted_class_idx]\n",
    "\n",
    "class_names = ['Violence', 'No Violence']\n",
    "predicted_class = class_names[predicted_class_idx]\n",
    "true_class = class_names[np.argmax(true_label)]\n",
    "\n",
    "print(f\"True Class: {true_class}\")\n",
    "print(f\"Predicted Class: {predicted_class}\")\n",
    "print(f\"Confidence: {confidence:.4f}\")\n",
    "print(f\"Probabilities: Violence={prediction[0]:.4f}, No Violence={prediction[1]:.4f}\")\n",
    "\n",
    "# Demo batch inference\n",
    "print(\"\\nBatch Inference Demo (first 5 test samples):\")\n",
    "batch_predictions = demo_model.predict(X_test[:5], verbose=0)\n",
    "\n",
    "for i in range(5):\n",
    "    pred = batch_predictions[i]\n",
    "    pred_class = class_names[np.argmax(pred)]\n",
    "    true_class = class_names[np.argmax(y_test[i])]\n",
    "    confidence = np.max(pred)\n",
    "    \n",
    "    status = \"✓\" if pred_class == true_class else \"✗\"\n",
    "    print(f\"Sample {i+1}: {status} True: {true_class}, Pred: {pred_class} ({confidence:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model architecture diagram (if available)\n",
    "try:\n",
    "    from tensorflow.keras.utils import plot_model\n",
    "    \n",
    "    # Create model diagram\n",
    "    plot_model(\n",
    "        demo_model,\n",
    "        to_file='model_architecture.png',\n",
    "        show_shapes=True,\n",
    "        show_layer_names=True,\n",
    "        rankdir='TB',\n",
    "        expand_nested=False,\n",
    "        dpi=96\n",
    "    )\n",
    "    \n",
    "    print(\"Model architecture diagram saved as 'model_architecture.png'\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"graphviz not available for model visualization\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not create model diagram: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model performance\n",
    "print(\"Performance Analysis:\")\n",
    "\n",
    "# Model size and parameters\n",
    "total_params = demo_model.count_params()\n",
    "model_size_mb = total_params * 4 / (1024 * 1024)  # Approximate size in MB\n",
    "\n",
    "print(f\"Model Parameters: {total_params:,}\")\n",
    "print(f\"Approximate Model Size: {model_size_mb:.2f} MB\")\n",
    "\n",
    "# Inference time analysis\n",
    "import time\n",
    "\n",
    "# Time single prediction\n",
    "start_time = time.time()\n",
    "_ = demo_model.predict(X_test[0:1], verbose=0)\n",
    "single_inference_time = time.time() - start_time\n",
    "\n",
    "# Time batch prediction\n",
    "start_time = time.time()\n",
    "_ = demo_model.predict(X_test, verbose=0)\n",
    "batch_inference_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nInference Performance:\")\n",
    "print(f\"Single prediction time: {single_inference_time:.4f} seconds\")\n",
    "print(f\"Batch prediction time ({len(X_test)} samples): {batch_inference_time:.4f} seconds\")\n",
    "print(f\"Average time per sample: {batch_inference_time / len(X_test):.4f} seconds\")\n",
    "print(f\"Throughput: {len(X_test) / batch_inference_time:.1f} samples/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Structure Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display project structure\n",
    "from utils import print_project_structure, validate_project_setup\n",
    "\n",
    "print(\"Project Structure:\")\n",
    "print_project_structure(project_root, max_depth=3)\n",
    "\n",
    "print(\"\\nProject Validation:\")\n",
    "validation = validate_project_setup(project_root)\n",
    "\n",
    "print(f\"Setup Complete: {validation['setup_complete']}\")\n",
    "print(f\"All Directories Exist: {validation['all_directories_exist']}\")\n",
    "print(f\"All Files Exist: {validation['all_files_exist']}\")\n",
    "print(f\"Dependencies Available: {validation['dependencies']['all_available']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"VIOLENCE DETECTION MVP - USAGE EXAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\"\"\n",
    "## 1. Training a Model\n",
    "\n",
    "```python\n",
    "from src.training import TrainingPipeline\n",
    "from src.config import Config\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize training pipeline\n",
    "config = Config()\n",
    "trainer = TrainingPipeline(config)\n",
    "\n",
    "# Train model\n",
    "data_dir = Path(\"data/raw\")\n",
    "train_data, train_targets, test_data, test_targets = trainer.prepare_data(data_dir)\n",
    "history = trainer.train_model(train_data, train_targets, test_data, test_targets)\n",
    "```\n",
    "\n",
    "## 2. Evaluating a Model\n",
    "\n",
    "```python\n",
    "from src.evaluation import ModelEvaluator\n",
    "from pathlib import Path\n",
    "\n",
    "# Load and evaluate model\n",
    "evaluator = ModelEvaluator(config)\n",
    "evaluator.load_model(Path(\"models/violence_detection_model.h5\"))\n",
    "results = evaluator.evaluate_model_comprehensive(test_data, test_targets)\n",
    "```\n",
    "\n",
    "## 3. Making Predictions\n",
    "\n",
    "```python\n",
    "from src.inference import ViolencePredictor\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize predictor\n",
    "predictor = ViolencePredictor(Path(\"models/violence_detection_model.h5\"))\n",
    "\n",
    "# Predict single video\n",
    "result = predictor.predict_video(Path(\"path/to/video.avi\"))\n",
    "print(f\"Violence detected: {result['violence_detected']}\")\n",
    "print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "```\n",
    "\n",
    "## 4. Visualization\n",
    "\n",
    "```python\n",
    "from src.visualization import TrainingVisualizer, EvaluationVisualizer\n",
    "\n",
    "# Plot training curves\n",
    "training_viz = TrainingVisualizer()\n",
    "training_viz.plot_training_history(history.history)\n",
    "\n",
    "# Plot confusion matrix\n",
    "eval_viz = EvaluationVisualizer()\n",
    "eval_viz.plot_confusion_matrix(confusion_matrix)\n",
    "```\n",
    "\n",
    "## 5. Real-time Processing\n",
    "\n",
    "```python\n",
    "from src.inference import RealTimeVideoProcessor\n",
    "\n",
    "# Process video stream\n",
    "processor = RealTimeVideoProcessor(Path(\"models/violence_detection_model.h5\"))\n",
    "processor.process_video_stream(\"path/to/video.mp4\", display=True)\n",
    "```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"NEXT STEPS FOR REAL DATA\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\"\"\n",
    "To use this MVP with real video data:\n",
    "\n",
    "1. **Prepare Data**:\n",
    "   - Place video files in data/raw/ directory\n",
    "   - Use naming convention:\n",
    "     * Violence: 'fi_*.avi', 'V_*.avi'\n",
    "     * No Violence: 'no_*.avi', 'NV_*.avi'\n",
    "\n",
    "2. **Train Model**:\n",
    "   ```python\n",
    "   from src.training import TrainingPipeline\n",
    "   trainer = TrainingPipeline()\n",
    "   trainer.prepare_data(Path(\"data/raw\"))\n",
    "   # Training will automatically extract features and cache them\n",
    "   ```\n",
    "\n",
    "3. **Evaluate Performance**:\n",
    "   ```python\n",
    "   from src.evaluation import evaluate_model_from_cache\n",
    "   results = evaluate_model_from_cache(\n",
    "       Path(\"models/violence_detection_model.h5\"),\n",
    "       Path(\"data/processed/test_features.h5\")\n",
    "   )\n",
    "   ```\n",
    "\n",
    "4. **Deploy for Inference**:\n",
    "   ```python\n",
    "   from src.inference import InferenceAPI\n",
    "   api = InferenceAPI(Path(\"models/violence_detection_model.h5\"))\n",
    "   result = api.predict_single_video(\"new_video.avi\")\n",
    "   ```\n",
    "\n",
    "5. **Monitor and Improve**:\n",
    "   - Use visualization tools to analyze performance\n",
    "   - Experiment with hyperparameters\n",
    "   - Collect more data for better accuracy\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"DEMO COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}