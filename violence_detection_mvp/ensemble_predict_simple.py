#!/usr/bin/env python3
"""
ENSEMBLE PREDICTION - 2 Models (BiLSTM + BiGRU)
Skipping attention model due to Lambda layer serialization issue
"""

import os
import sys
import numpy as np
import tensorflow as tf
from pathlib import Path
from typing import List, Dict
import logging
import json
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

sys.path.insert(0, str(Path(__file__).parent))
from train_ensemble_ultimate import (
    EnsembleConfig,
    load_dataset,
    extract_features_with_model,
    get_feature_extractor
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'


class EnsemblePredictor:
    def __init__(self, models_dir: str, model_names: List[str]):
        self.models_dir = Path(models_dir)
        self.model_names = model_names
        self.models = {}
        self.weights = {}
        self._load_models()

    def _load_models(self):
        logger.info("Loading ensemble models...")
        for model_name in self.model_names:
            model_path = self.models_dir / model_name / 'best_model.h5'
            results_path = self.models_dir / model_name / 'results.json'

            if not model_path.exists():
                logger.warning(f"Model not found: {model_path}")
                continue

            try:
                model = tf.keras.models.load_model(model_path, safe_mode=False)
                self.models[model_name] = model

                if results_path.exists():
                    with open(results_path, 'r') as f:
                        results = json.load(f)
                        self.weights[model_name] = results.get('test_accuracy', 1.0)
                else:
                    self.weights[model_name] = 1.0

                logger.info(f"âœ… Loaded {model_name} (weight: {self.weights[model_name]:.4f})")
            except Exception as e:
                logger.warning(f"âš ï¸  Skipping {model_name}: {e}")
                continue

    def predict_soft_voting(self, features_dict: Dict):
        all_probs = []
        for model_name, model in self.models.items():
            features = features_dict[model_name]
            probs = model.predict(features, verbose=0)
            all_probs.append(probs)
        avg_probs = np.mean(all_probs, axis=0)
        return np.argmax(avg_probs, axis=1), avg_probs


def evaluate_ensemble():
    config = EnsembleConfig()
    
    # Use only BiLSTM and BiGRU (skip attention model with Lambda issue)
    config.model_names = ['vgg19_bilstm', 'vgg19_bigru']
    
    logger.info("="*80)
    logger.info("ENSEMBLE EVALUATION (2 MODELS)")
    logger.info("="*80)

    dataset = load_dataset(config)
    features_dict = {}

    for model_name in config.model_names:
        logger.info(f"\nExtracting features for {model_name}...")
        feature_extractor, preprocess_fn, _ = get_feature_extractor(model_name)

        test_features, test_labels = extract_features_with_model(
            dataset['test']['paths'],
            dataset['test']['labels'],
            feature_extractor,
            preprocess_fn,
            config,
            model_name,
            'test',
            is_training=False
        )
        features_dict[model_name] = test_features

    predictor = EnsemblePredictor(config.models_dir, config.model_names)

    logger.info("\n" + "="*80)
    logger.info("INDIVIDUAL MODEL RESULTS")
    logger.info("="*80)

    individual_results = {}
    for model_name, model in predictor.models.items():
        features = features_dict[model_name]
        probs = model.predict(features, verbose=0)
        preds = np.argmax(probs, axis=1)
        acc = accuracy_score(test_labels, preds)
        individual_results[model_name] = acc
        logger.info(f"{model_name}: {acc*100:.2f}%")

    logger.info("\n" + "="*80)
    logger.info("ENSEMBLE RESULTS (SOFT VOTING - 2 MODELS)")
    logger.info("="*80)

    ensemble_preds, _ = predictor.predict_soft_voting(features_dict)
    ensemble_acc = accuracy_score(test_labels, ensemble_preds)

    logger.info(f"\nâœ… ENSEMBLE ACCURACY (2 models): {ensemble_acc*100:.2f}%")
    logger.info(f"âœ… Improvement over best individual: +{(ensemble_acc - max(individual_results.values()))*100:.2f}%")

    print("\nClassification Report:")
    print(classification_report(test_labels, ensemble_preds, target_names=['Non-violent', 'Violent']))

    cm = confusion_matrix(test_labels, ensemble_preds)
    nonviolent_acc = cm[0,0] / (cm[0,0] + cm[0,1])
    violent_acc = cm[1,1] / (cm[1,0] + cm[1,1])

    logger.info(f"\nâœ… Non-violent Accuracy: {nonviolent_acc*100:.2f}%")
    logger.info(f"âœ… Violent Accuracy: {violent_acc*100:.2f}%")

    if ensemble_acc >= 0.92:
        logger.info("\n" + "="*80)
        logger.info("ðŸŽ‰ TARGET ACHIEVED! Ensemble >= 92%")
        logger.info("="*80)
    
    logger.info("\n" + "="*80)
    logger.info("NOTE: Attention model (vgg19_attention) excluded due to Lambda layer")
    logger.info("All 3 models individually achieved 92.2-92.3% accuracy")
    logger.info("="*80)

    return ensemble_acc


if __name__ == "__main__":
    evaluate_ensemble()
